### 2019-11-22:

0.1版正式完成。解决了第一次写的时候的内存泄露、对象生命周期意外延长等问题。

第一版核心部分虽然具体代码实现不一样，但结构上主要都是muduo的设计(照猫画虎)，HTTP的解析和超时管理部分才比较有我自己的想法。

希望后续再从nginx学到一些东西。

### 2019-12-10

在4核CPU，4G内存的虚拟机Ubuntu中，利用ApacheBench进行了压测，测试结果见imgs目录。

分别用4线程(进程)下的muduo、nginx以及本项目实现简单的GET请求，从内存直接响应一个“Hello World”。分别测试开启和关闭keep-alive选项。

利用本地环境测试1000并发下，发送200000个请求的表现。 

    ab -n 200000 -c 1000 -m GET [-k] URL

测试过程不严谨，各个项目响应头的大小不一致，响应内容也很简单，日志也没完全关闭。没有对比的意义，仅仅是看一个并发性能上的大致表现。

短连接表现还不错，keep-alive选项开启后稍差一点。

### 2019-12-25

0.2版。主要做了以下改进：

* 添加时间缓存，避免每次添加定时器都进行系统调用clock\_gettime。

* 重新实现了时间堆，放弃不能修改\删除堆中项的priority\_queue，避免采用逻辑删除带来的内存膨胀。

* 在服务器启动时建立HttpConn连接池，避免每次创建、销毁对象的开销。

* 加入了一个简单的负载检查。

* 修改了一些HttpConn里的代码实现。

1、时间缓存是看了nginx得到的启发，但是nginx采用setitimer，用SIGALARM信号处理来做，这在多线程服务器里显然不适用。
由于目前定时器只做超时连接检查的工作，所以精度上的要求可以降低一点。模仿了libevent的做法，在每次loop开始时重置时间缓存标志。这次loop内的第一次获取时间就会实际的调用系统函数。

2、原本时间堆用STL的优先队列实现，但是STL中无论是优先队列还是heap系列函数都无法支持修改、删除堆里面的元素。逻辑删除虽然可行，但是请求多了内存膨胀，而且超时后弹出那些
被逻辑删除的定时器也会浪费时间，所以手写重新实现了时间堆，支持实际的修改、删除操作。

3、看了nginx得到的启发，预先创建HttpConn池，这样避免频繁的创建和销毁对象。但是目前是根据最大fd建立池，根据建立连接后的fd取HttpConn，这么做在并发级别小的时候内存利用率应该很低。未来会不会引发更多问题也未知。

4、学习nginx加入一个简单的负载检查，大概思想就是如果某个thread连接数较多，Round Robin的时候就跳过他，算是一种简单的负载均衡的做法。

修改前后对比表现还是提升不少，特别是processing time和waiting time都有下降。虽然可能也有虚拟机性能不稳定的因素影响。

在nginx，muduo以及本项目的服务器做了一个更详细的测试对比，见readme。

### 2020-01-09

又做了一些改进：

1、定时器中，我原先居然想用一个long存绝对时间毫秒，可想而知会发生越界，最后就导致线程死在timerfd\_settime。改成了和系统里面一样的秒和毫秒的结构体方式。

2、修改连接池的写法，现在和nginx一样是链表，这样可以不用预先创建MAXFD的池，并发小的时候节省了内存。

3、改进了HttpConn的一些方法，现在处理方式更合理一些，也加入了一些其他的图片之类的类型。

4、增加了Post请求的处理，在index.html中实现一个简单的登陆注册的功能，可以存到mysql数据库中，不过做的非常简陋，也没有做成mysql连接池。试过写连接池，但是发现即使不做查询，仅仅维持连接，服务器性能也严重受到影响，有时间再研究原因。

PS：也试图实现cgi，但是pipe-fork-exec一直调不通，有时间解决。

## <a name='bench'>压测</a>

修改后重新做了压测，简单分析一下：

* 开启keep-alive

nginx虽然QPS低一点点，但是事实上因为nginx自带的响应头比较大，看transfer rate比muduo和webserver都高了很多，而muduo和webserver差不多。
猜测是因为muduo和webserver在处理时都用了标准库容器和算法，虽然编程上更方便，但效率和内存占用上不如用C实现的nginx。
由于长连接省去了建立连接，销毁连接等开销，只进行I/O，在都用4线程/进程+epoll的情况下差别不算大。而且测试时业务简单，数据量很小，epoll工作在LT模式和ET模式差距不大。

* 不开启keep-alive

对比muduo，muduo使用的是LT模式的监听套接字，每次只接受一个。在短连接高并发测试时，频繁有新连接进来的情况下，会导致很多连接等待。并且muduo没有连接池，频繁的创建销毁对象也会是大问题。
1000并发下，muduo的connect平均要用20ms+，而本项目仅要15ms。而且用top命令观察资源使用也会发现，要并发时muduo的CPU占用率最高的是MainReactor，而4个SubReacotr子线程占用率都不超过50%，说明服务器浪费大量时间在建立连接上。

对比nginx，在并发不大的时候nginx还是很好。但是随着并发级别越来越高，nginx也表现不佳。虽然nginx也是采用非阻塞套接字+ET触发模式，但是为了能平滑重启，重启后不丢失等待建立连接的队列，
nginx对监听套接字单独采用了水平触发，所以高并发短连接下的性能也不好。

webserver采用ET触发+非阻塞，每次都用一个while循环accept完所有连接，高并发下效率较好。对比muduo又有连接池的设计，可以避免频繁创建/销毁对象。

不过归根结底，测试不严谨，虚拟机环境不稳定，这个结果参考意义也有限。

#### TODO: 

#### 完善文档。

#### CGI。
